{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92533677",
   "metadata": {},
   "source": [
    "# Time Series Forecasting usinhg Transformer: Single-time-step Predictions\n",
    "\n",
    "\n",
    "In this notebook, we describe the steps for single-time-step predictions for a wesather forecasting problem. We train a Transformer model for making predictions.\n",
    "\n",
    "The notebook is adapted from the following two Keras tutorials.\n",
    "- https://keras.io/examples/timeseries/timeseries_weather_forecasting/\n",
    "- https://keras.io/examples/timeseries/timeseries_classification_transformer/\n",
    "\n",
    "\n",
    "## Dataset\n",
    "We use the the Jena Climate dataset by the Max Planck Institute for Biogeochemistry.\n",
    "https://www.bgc-jena.mpg.de/wetter/\n",
    "\n",
    "The dataset has 15 features such as datetime, temperature, pressure, humidity, etc. There are 420551 observations (time steps), each recorded once per 10 minutes (6 times per hour).\n",
    "\n",
    "- Total observations = 420551 (an observation is recorded per 10 minutes)\n",
    "- Total features = 15\n",
    "\n",
    "Though 6 observations are recorded every hour, we use only one observation per hour to train the model since no drastic change is expected within an hour. \n",
    "\n",
    "\n",
    "## Single-time-step Predictions\n",
    "\n",
    "For the single-time-step predictions, we need to decide the length of each sequence and the a future location on the sequence that we want to predict.\n",
    "\n",
    "We chose the sequence length to be 120 hours (on the 10 minutes recording scale, the length spans 720 observations).\n",
    "\n",
    "We want to predict an observation which is 12 hours (on the 10 minutes recording scale, it is 72 observations) future from the last observation in the sequence. This gap is added to make the prediction slightly non-trivial. \n",
    "\n",
    "The first training sequence is as follows (though we use only one observation per hour):\n",
    "\n",
    "        0 1 2 ... 719\n",
    "\n",
    "The index of the location of the first prediction (720 training observations + 72 \"gap\" observations):\n",
    "        \n",
    "        792\n",
    "\n",
    "\n",
    "\n",
    "For training the model, we use ~300,000 samples (observations). These observations are divided into constant length sequences.\n",
    "- Each sequence length = 120 hours (720 observations)\n",
    "- Every sequence has a label (after the 72 observations)\n",
    "\n",
    "Therefore,\n",
    "- A sample = 120 hours (720 observations) + 1 label\n",
    "- A batch contains multiple samples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbf4d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipfile import ZipFile\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ba1547",
   "metadata": {},
   "source": [
    "## Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a0a06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Download the Dataset\n",
    "'''\n",
    "uri = \"https://storage.googleapis.com/tensorflow/tf-keras-datasets/jena_climate_2009_2016.csv.zip\"\n",
    "zip_path = tf.keras.utils.get_file(origin=uri, fname=\"jena_climate_2009_2016.csv.zip\")\n",
    "zip_file = ZipFile(zip_path)\n",
    "zip_file.extractall()\n",
    "csv_path = \"jena_climate_2009_2016.csv\"\n",
    "\n",
    "\n",
    "'''\n",
    "Load the dataset as a Pandas DataFRame object\n",
    "'''\n",
    "df = pd.read_csv(csv_path)\n",
    "print(\"Dimension of the data: \", df.shape)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8916f497",
   "metadata": {},
   "source": [
    "## Raw Data Visualization\n",
    "\n",
    "We plot each feature pattern over the time period from 2009 to 2016. The plots help to identify the anomalies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb4f32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = [\n",
    "    \"Pressure\",\n",
    "    \"Temperature\",\n",
    "    \"Temperature in Kelvin\",\n",
    "    \"Temperature (dew point)\",\n",
    "    \"Relative Humidity\",\n",
    "    \"Saturation vapor pressure\",\n",
    "    \"Vapor pressure\",\n",
    "    \"Vapor pressure deficit\",\n",
    "    \"Specific humidity\",\n",
    "    \"Water vapor concentration\",\n",
    "    \"Airtight\",\n",
    "    \"Wind speed\",\n",
    "    \"Maximum wind speed\",\n",
    "    \"Wind direction in degrees\",\n",
    "]\n",
    "\n",
    "feature_keys = [\n",
    "    \"p (mbar)\",\n",
    "    \"T (degC)\",\n",
    "    \"Tpot (K)\",\n",
    "    \"Tdew (degC)\",\n",
    "    \"rh (%)\",\n",
    "    \"VPmax (mbar)\",\n",
    "    \"VPact (mbar)\",\n",
    "    \"VPdef (mbar)\",\n",
    "    \"sh (g/kg)\",\n",
    "    \"H2OC (mmol/mol)\",\n",
    "    \"rho (g/m**3)\",\n",
    "    \"wv (m/s)\",\n",
    "    \"max. wv (m/s)\",\n",
    "    \"wd (deg)\",\n",
    "]\n",
    "\n",
    "colors = [\n",
    "    \"blue\",\n",
    "    \"orange\",\n",
    "    \"green\",\n",
    "    \"red\",\n",
    "    \"purple\",\n",
    "    \"brown\",\n",
    "    \"pink\",\n",
    "    \"gray\",\n",
    "    \"olive\",\n",
    "    \"cyan\",\n",
    "]\n",
    "\n",
    "date_time_key = \"Date Time\"\n",
    "\n",
    "\n",
    "def show_raw_visualization(data):\n",
    "    time_data = data[date_time_key]\n",
    "    fig, axes = plt.subplots(\n",
    "        nrows=7, ncols=2, figsize=(15, 20), dpi=80, facecolor=\"w\", edgecolor=\"k\"\n",
    "    )\n",
    "    for i in range(len(feature_keys)):\n",
    "        key = feature_keys[i]\n",
    "        c = colors[i % (len(colors))]\n",
    "        t_data = data[key]\n",
    "        t_data.index = time_data\n",
    "        t_data.head()\n",
    "        ax = t_data.plot(\n",
    "            ax=axes[i // 2, i % 2],\n",
    "            color=c,\n",
    "            title=\"{} - {}\".format(titles[i], key),\n",
    "            rot=25,\n",
    "        )\n",
    "        ax.legend([titles[i]])\n",
    "    plt.tight_layout()\n",
    "\n",
    "\n",
    "show_raw_visualization(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0ab017",
   "metadata": {},
   "source": [
    "## Feature Correlation\n",
    "\n",
    "We create a heat map to show the correlation between different features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e61ee0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_heatmap(data):\n",
    "    plt.matshow(data.corr())\n",
    "    plt.xticks(range(data.shape[1]), data.columns, fontsize=14, rotation=90)\n",
    "    plt.gca().xaxis.tick_bottom()\n",
    "    plt.yticks(range(data.shape[1]), data.columns, fontsize=14)\n",
    "\n",
    "    cb = plt.colorbar()\n",
    "    cb.ax.tick_params(labelsize=14)\n",
    "    plt.title(\"Feature Correlation Heatmap\", fontsize=14)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "show_heatmap(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe6ade5",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "As mentioned earlier, we use one obervation per hour by resampling from 6 observations/hour.\n",
    "\n",
    "Since the length of a sequence is 120, we use 720 observations-long sequence for prediction (training). The prediction location on the sequence is after 12 hours or 72 observations. This observation is used as a label.\n",
    "\n",
    "\n",
    "### Normalization\n",
    "Since every feature has values with varying ranges, we do normalization to confine feature values to a range of [0, 1] before training a neural network. We do this by subtracting the mean and dividing by the standard deviation of each feature.\n",
    "\n",
    "### Train-Test Split\n",
    "71.5 % of the data will be used to train the model, i.e. 300,693 rows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afde0059",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_fraction = 0.715\n",
    "train_split = int(split_fraction * int(df.shape[0]))\n",
    "step = 6  # It is required to determine the length of a sequence\n",
    "\n",
    "past = 720  # Length of a sequence on the original observations (5 days)\n",
    "future = 72  # Prediction is made after 72 observations (12 hours)\n",
    "\n",
    "learning_rate = 0.001\n",
    "batch_size = 256\n",
    "epochs = 50\n",
    "\n",
    "\n",
    "def normalize(data, train_split):\n",
    "    data_mean = data[:train_split].mean(axis=0)\n",
    "    data_std = data[:train_split].std(axis=0)\n",
    "    return (data - data_mean) / data_std"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0cefefc",
   "metadata": {},
   "source": [
    "## Feature Selection\n",
    "\n",
    "From the correlation heatmap, we identify that few parameters like Relative Humidity and Specific Humidity are redundant. This, we select only the relevant features for learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92f3272",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    \"The selected parameters are:\",\n",
    "    \", \".join([titles[i] for i in [0, 1, 5, 7, 8, 10, 11]]),\n",
    ")\n",
    "selected_features = [feature_keys[i] for i in [0, 1, 5, 7, 8, 10, 11]]\n",
    "features = df[selected_features]\n",
    "features.index = df[date_time_key]\n",
    "features.head()\n",
    "\n",
    "features = normalize(features.values, train_split)\n",
    "features = pd.DataFrame(features)\n",
    "features.head()\n",
    "\n",
    "train_data = features.loc[0 : train_split - 1]\n",
    "val_data = features.loc[train_split:]\n",
    "\n",
    "print(\"All data: \", features.shape)\n",
    "print(\"Train data: \", train_data.shape)\n",
    "print(\"Val data: \", val_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b716fbd",
   "metadata": {},
   "source": [
    "## Create Train & Validation Datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e20b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = past + future # Defines the start position of the labels, i.e., from the 792nd observation (720 + 72)\n",
    "end = start + train_split # Defines the end position of the labels\n",
    "\n",
    "'''\n",
    "Creates train data and labels\n",
    "'''\n",
    "x_train = train_data[[i for i in range(7)]].values\n",
    "y_train = features.iloc[start:end][[1]] # Labels start from the 792nd observation (720 + 72)\n",
    "\n",
    "print(\"\\nx_train shape: \", x_train.shape)\n",
    "print(\"y_train shape: \", y_train.shape)\n",
    "\n",
    "\n",
    "'''\n",
    "Defines the length of a sequence:\n",
    "- (720 observations in a sequence) divided by (6 observations per hour)\n",
    "'''\n",
    "sequence_length = int(past / step)\n",
    "print(\"\\nsequence_length: \", sequence_length)\n",
    "\n",
    "'''\n",
    "Create a Dataset object for the training data.\n",
    "For this, we use the \n",
    "keras.preprocessing.timeseries_dataset_from_array function.\n",
    "It takes in a sequence of data-points gathered at equal intervals, \n",
    "along with time series parameters such as length of the sequences/windows, \n",
    "spacing between two sequence/windows, etc., to produce batches of sub-timeseries inputs and \n",
    "targets sampled from the main timeseries.\n",
    "'''\n",
    "\n",
    "dataset_train = tf.keras.preprocessing.timeseries_dataset_from_array(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    sequence_length=sequence_length,\n",
    "    sampling_rate=step,\n",
    "    batch_size=batch_size,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "Create the validation data\n",
    "'''\n",
    "x_end = len(val_data) - past - future\n",
    "label_start = train_split + past + future\n",
    "\n",
    "\n",
    "x_val = val_data.iloc[:x_end][[i for i in range(7)]].values\n",
    "y_val = features.iloc[label_start:][[1]]\n",
    "\n",
    "\n",
    "'''\n",
    "Create a Dataset object for the training data.\n",
    "For this, we use the \n",
    "keras.preprocessing.timeseries_dataset_from_array function.\n",
    "'''\n",
    "dataset_val = tf.keras.preprocessing.timeseries_dataset_from_array(\n",
    "    x_val,\n",
    "    y_val,\n",
    "    sequence_length=sequence_length,\n",
    "    sampling_rate=step,\n",
    "    batch_size=batch_size,\n",
    ")\n",
    "\n",
    "\n",
    "for batch in dataset_train.take(1):\n",
    "    inputs, targets = batch\n",
    "\n",
    "print(\"\\nInput shape:\", inputs.numpy().shape)\n",
    "print(\"Target shape:\", targets.numpy().shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27bd14b6",
   "metadata": {},
   "source": [
    "## Create a Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d108f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
    "    # Attention and Normalization\n",
    "    x = tf.keras.layers.MultiHeadAttention(\n",
    "        key_dim=head_size, num_heads=num_heads, dropout=dropout\n",
    "    )(inputs, inputs) \n",
    "    x = tf.keras.layers.Dropout(dropout)(x)\n",
    "    x = tf.keras.layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "    res = x + inputs\n",
    "\n",
    "    # Feed Forward Part\n",
    "    x = tf.keras.layers.Conv1D(filters=ff_dim, kernel_size=1, activation=\"relu\")(res)\n",
    "    x = tf.keras.layers.Dropout(dropout)(x)\n",
    "    x = tf.keras.layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)\n",
    "    x = tf.keras.layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "    \n",
    "    return x + res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b26b2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(\n",
    "    input_shape,\n",
    "    head_size,\n",
    "    num_heads,\n",
    "    ff_dim,\n",
    "    num_transformer_blocks,\n",
    "    mlp_units,\n",
    "    dropout=0,\n",
    "    mlp_dropout=0,\n",
    "):\n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "    x = inputs\n",
    "    for _ in range(num_transformer_blocks):\n",
    "        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)\n",
    "\n",
    "    x = tf.keras.layers.GlobalAveragePooling1D(data_format=\"channels_first\")(x)\n",
    "    for dim in mlp_units:\n",
    "        x = tf.keras.layers.Dense(dim, activation=\"relu\")(x)\n",
    "        x = tf.keras.layers.Dropout(mlp_dropout)(x)\n",
    "        \n",
    "    # Output layer\n",
    "    outputs = tf.keras.layers.Dense(1, activation=None)(x)\n",
    "    \n",
    "    return tf.keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce26ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in dataset_train.take(1):\n",
    "    inputs, targets = batch\n",
    "\n",
    "print(\"\\nInput shape:\", inputs.numpy().shape)\n",
    "print(\"Target shape:\", targets.numpy().shape)\n",
    "\n",
    "input_shape = inputs.numpy().shape[1:]\n",
    "\n",
    "print(\"Transformer input_shape: \", input_shape)\n",
    "\n",
    "model_transformer = build_model(\n",
    "    input_shape,\n",
    "    head_size=256,\n",
    "    num_heads=4,\n",
    "    ff_dim=4,\n",
    "    num_transformer_blocks=4,\n",
    "    mlp_units=[128],\n",
    "    mlp_dropout=0.4,\n",
    "    dropout=0.25,\n",
    ")\n",
    "\n",
    "\n",
    "model_transformer.compile(loss=tf.keras.losses.MeanSquaredError(),\n",
    "                optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "                metrics=[tf.keras.metrics.MeanAbsoluteError()])\n",
    "\n",
    "model_transformer.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829e2401",
   "metadata": {},
   "source": [
    "## Train the Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6890aac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_checkpoint = \"Weather_forecasting_half_day_Transformer_checkpoint.h5\"\n",
    "es_callback = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", min_delta=0, patience=5)\n",
    "\n",
    "modelckpt_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    monitor=\"val_loss\",\n",
    "    filepath=path_checkpoint,\n",
    "    verbose=1,\n",
    "    save_weights_only=True,\n",
    "    save_best_only=True,\n",
    ")\n",
    "\n",
    "history_transformer = model_transformer.fit(\n",
    "    dataset_train,\n",
    "    epochs=epochs,\n",
    "    validation_data=dataset_val,\n",
    "    callbacks=[es_callback, modelckpt_callback],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49109c0e",
   "metadata": {},
   "source": [
    "## Visualize the Train & Validation Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2657dd71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_loss(history, title):\n",
    "    loss = history.history[\"loss\"]\n",
    "    val_loss = history.history[\"val_loss\"]\n",
    "    epochs = range(len(loss))\n",
    "    plt.figure()\n",
    "    plt.plot(epochs, loss, \"b\", label=\"Training loss\")\n",
    "    plt.plot(epochs, val_loss, \"r\", label=\"Validation loss\")\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "visualize_loss(history_transformer, \"Training and Validation Loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c75532b",
   "metadata": {},
   "source": [
    "## Model Evaluation: using the History Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affeb265",
   "metadata": {},
   "outputs": [],
   "source": [
    "numOfEpochs = len(history_transformer.history['loss'])\n",
    "print(\"Epochs: \", numOfEpochs)\n",
    "\n",
    "train_MAE = history_transformer.history['mean_absolute_error']\n",
    "val_MAE = history_transformer.history['val_mean_absolute_error']\n",
    "\n",
    "\n",
    "train_loss = history_transformer.history['loss']\n",
    "val_loss = history_transformer.history['val_loss']\n",
    "\n",
    "\n",
    "# Read the last value from the list that represents final epoch statistics\n",
    "print(\"\\nMAE (train): \", train_MAE[-1])\n",
    "print(\"MAE (val): \", val_MAE[-1])\n",
    "\n",
    "print(\"\\nLoss (train): \", train_loss[-1])\n",
    "print(\"Loss (val): \", val_loss[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bfeb145",
   "metadata": {},
   "source": [
    "## Model Evaluation: using the evaluate() Method on the Batched Val Dataset Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875d37f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss_dataset, val_MAE_dataset = model.evaluate(dataset_val, verbose=0)\n",
    "\n",
    "print(\"\\nLoss (val) from Dataset: \", val_loss_dataset)\n",
    "print(\"MAE (val) from Dataset: \", val_MAE_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55957c63",
   "metadata": {},
   "source": [
    "## Prediction\n",
    "\n",
    "We make predictions for 5 sets of values from validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a513e1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_plot(plot_data, delta, title):\n",
    "    labels = [\"History\", \"True Future\", \"Model Prediction\"]\n",
    "    marker = [\".-\", \"rx\", \"go\"]\n",
    "    time_steps = list(range(-(plot_data[0].shape[0]), 0))\n",
    "    if delta:\n",
    "        future = delta\n",
    "    else:\n",
    "        future = 0\n",
    "\n",
    "    plt.title(title)\n",
    "    for i, val in enumerate(plot_data):\n",
    "        if i:\n",
    "            plt.plot(future, plot_data[i], marker[i], markersize=10, label=labels[i])\n",
    "        else:\n",
    "            plt.plot(time_steps, plot_data[i].flatten(), marker[i], label=labels[i])\n",
    "    plt.legend()\n",
    "    plt.xlim([time_steps[0], (future + 5) * 2])\n",
    "    plt.xlabel(\"Time-Step\")\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "\n",
    "# The future step for plotting the prections\n",
    "future_step = int(future/step)\n",
    "\n",
    "for x, y in dataset_val.take(5):\n",
    "    show_plot(\n",
    "        [x[0][:, 1].numpy(), y[0].numpy(), model_transformer.predict(x)[0]],\n",
    "        future_step,\n",
    "        \"Single Step Prediction\",\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
